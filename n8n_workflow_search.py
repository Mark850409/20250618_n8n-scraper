#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
n8n Workflow æ™ºèƒ½æœå°‹èˆ‡æ•™å­¸ç”Ÿæˆç³»çµ±
åŸºæ–¼ LangChain + ChromaDB çš„å‘é‡æœå°‹å¼•æ“
"""

import os
import json
import shutil
from typing import List, Dict, Any, Tuple
from datetime import datetime
import chromadb
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class N8nWorkflowSearcher:
    """n8n Workflow æœå°‹èˆ‡æ•™å­¸ç”Ÿæˆç³»çµ±"""
    
    def __init__(self, workflows_dir: str = ".", chroma_db_path: str = "./chroma_db"):
        """
        åˆå§‹åŒ–æœå°‹ç³»çµ±
        
        Args:
            workflows_dir: workflow JSON æª”æ¡ˆæ‰€åœ¨ç›®éŒ„
            chroma_db_path: ChromaDB è³‡æ–™åº«è·¯å¾‘
        """
        self.workflows_dir = workflows_dir
        self.chroma_db_path = chroma_db_path
        self.selected_dir = "selected_workflows"
        
        # å»ºç«‹å¿…è¦ç›®éŒ„
        os.makedirs(self.selected_dir, exist_ok=True)
        os.makedirs(chroma_db_path, exist_ok=True)
        
        # åˆå§‹åŒ– Embedding æ¨¡å‹
        logger.info("æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'}
        )
        
        # åˆå§‹åŒ– ChromaDB
        self.chroma_client = chromadb.PersistentClient(path=chroma_db_path)
        self.collection_name = "n8n_workflows"
        
        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        self.vectorstore = None
        self.init_vectorstore()
    
    def init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡è³‡æ–™åº«"""
        try:
            # å˜—è©¦è¼‰å…¥ç¾æœ‰çš„å‘é‡è³‡æ–™åº«
            self.vectorstore = Chroma(
                collection_name=self.collection_name,
                embedding_function=self.embeddings,
                persist_directory=self.chroma_db_path
            )
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
            collection = self.chroma_client.get_collection(self.collection_name)
            count = collection.count()
            
            if count == 0:
                logger.info("å‘é‡è³‡æ–™åº«ç‚ºç©ºï¼Œé–‹å§‹å»ºç«‹ç´¢å¼•...")
                self.build_index()
            else:
                logger.info(f"è¼‰å…¥ç¾æœ‰å‘é‡è³‡æ–™åº«ï¼ŒåŒ…å« {count} ç­†è³‡æ–™")
                
        except Exception as e:
            logger.info("å»ºç«‹æ–°çš„å‘é‡è³‡æ–™åº«...")
            self.build_index()
    
    def extract_workflow_info(self, workflow_data: Dict[str, Any], filename: str) -> Dict[str, Any]:
        """
        å¾ workflow JSON ä¸­æå–é—œéµè³‡è¨Š
        
        Args:
            workflow_data: workflow JSON è³‡æ–™
            filename: æª”æ¡ˆåç¨±
            
        Returns:
            Dict: æå–çš„å·¥ä½œæµç¨‹è³‡è¨Š
        """
        # åŸºæœ¬è³‡è¨Š
        workflow_id = workflow_data.get('id', 'unknown')
        workflow_name = workflow_data.get('name', 'Unnamed Workflow')
        
        # ç¯€é»è³‡æ–™ç›´æ¥åœ¨æ ¹å±¤ç´šï¼ˆå› ç‚ºæˆ‘å€‘å·²ç¶“åªä¿å­˜ workflow å…§å®¹ï¼‰
        nodes = workflow_data.get('nodes', [])
        
        # æå–ç¯€é»çµæ§‹
        node_structure = []
        for node in nodes:
            node_name = node.get('name', 'Unknown Node')
            node_type = node.get('type', 'unknown')
            node_structure.append(f"{node_name}ï¼ˆ{node_type}ï¼‰")
        
        # ç”Ÿæˆæè¿°æ–‡å­—ï¼ˆç”¨æ–¼æœå°‹ï¼‰
        description_parts = [
            workflow_name,
            f"å·¥ä½œæµç¨‹åŒ…å« {len(nodes)} å€‹ç¯€é»",
            "ç¯€é»é¡å‹ï¼š" + "ã€".join([node.get('type', 'unknown') for node in nodes[:5]])
        ]
        
        # æ ¹æ“šç¯€é»é¡å‹æ¨æ¸¬åŠŸèƒ½
        node_types = [node.get('type', '') for node in nodes]
        if any('trigger' in nt.lower() for nt in node_types):
            description_parts.append("åŒ…å«è§¸ç™¼å™¨")
        if any('http' in nt.lower() for nt in node_types):
            description_parts.append("åŒ…å«HTTPè«‹æ±‚")
        if any('email' in nt.lower() for nt in node_types):
            description_parts.append("åŒ…å«éƒµä»¶åŠŸèƒ½")
        if any('discord' in nt.lower() for nt in node_types):
            description_parts.append("åŒ…å«Discordé€šçŸ¥")
        if any('schedule' in nt.lower() for nt in node_types):
            description_parts.append("åŒ…å«å®šæ™‚æ’ç¨‹")
        
        return {
            'id': workflow_id,
            'name': workflow_name,
            'filename': filename,
            'description': "ï¼›".join(description_parts),
            'node_structure': node_structure,
            'node_count': len(nodes),
            'node_types': node_types,
            'raw_data': workflow_data
        }
    
    def build_index(self):
        """å»ºç«‹å‘é‡è³‡æ–™åº«ç´¢å¼•"""
        logger.info("é–‹å§‹æƒæ workflow æª”æ¡ˆ...")
        
        documents = []
        
        # æƒææ‰€æœ‰ JSON æª”æ¡ˆ
        json_files = []
        for root, dirs, files in os.walk(self.workflows_dir):
            for file in files:
                if file.endswith('.json') and not file.startswith('.'):
                    json_files.append(os.path.join(root, file))
        
        logger.info(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    workflow_data = json.load(f)
                
                # æå–å·¥ä½œæµç¨‹è³‡è¨Š
                workflow_info = self.extract_workflow_info(workflow_data, os.path.basename(json_file))
                
                # å»ºç«‹æ–‡æª”å…§å®¹ï¼ˆç”¨æ–¼å‘é‡æœå°‹ï¼‰
                doc_content = f"""
                å·¥ä½œæµç¨‹åç¨±ï¼š{workflow_info['name']}
                æè¿°ï¼š{workflow_info['description']}
                ç¯€é»çµæ§‹ï¼š{'; '.join(workflow_info['node_structure'][:10])}
                """
                
                # å»ºç«‹ Document ä¸¦åŒ…å« metadata
                doc_metadata = {
                    'filename': workflow_info['filename'],
                    'workflow_id': str(workflow_info['id']),
                    'name': workflow_info['name'],
                    'description': workflow_info['description'],
                    'node_structure': str(workflow_info['node_structure']),  # è½‰ç‚ºå­—ä¸²
                    'node_count': workflow_info['node_count'],
                    'file_path': json_file
                }
                documents.append(Document(page_content=doc_content, metadata=doc_metadata))
                
            except Exception as e:
                logger.warning(f"è™•ç†æª”æ¡ˆ {json_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        if documents:
            # å»ºç«‹å‘é‡è³‡æ–™åº«
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                collection_name=self.collection_name,
                persist_directory=self.chroma_db_path
            )
            
            logger.info(f"æˆåŠŸå»ºç«‹å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {len(documents)} ç­†è³‡æ–™")
        else:
            logger.error("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ workflow æª”æ¡ˆ")
    
    def search_workflows(self, query: str, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:
        """
        æœå°‹ç›¸ä¼¼çš„å·¥ä½œæµç¨‹
        
        Args:
            query: æœå°‹æŸ¥è©¢
            top_k: å›å‚³å‰ N ç­†çµæœ
            
        Returns:
            List: æœå°‹çµæœåˆ—è¡¨ï¼ŒåŒ…å« (metadata, similarity_score)
        """
        if not self.vectorstore:
            logger.error("å‘é‡è³‡æ–™åº«æœªåˆå§‹åŒ–")
            return []
        
        try:
            # åŸ·è¡Œç›¸ä¼¼åº¦æœå°‹
            results = self.vectorstore.similarity_search_with_score(query, k=top_k)

            search_results = []
            for doc, score in results:
                # å–å¾— metadata
                metadata = doc.metadata
                # ChromaDB ä½¿ç”¨é¤˜å¼¦è·é›¢ï¼Œè·é›¢è¶Šå°ç›¸ä¼¼åº¦è¶Šé«˜
                # å°‡è·é›¢è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
                if score <= 1.0:
                    similarity = 1.0 - score  # è·é›¢è½‰ç›¸ä¼¼åº¦
                else:
                    # è™•ç†å¯èƒ½çš„ç•°å¸¸è·é›¢å€¼
                    similarity = 1.0 / (1.0 + score)

                # ç¢ºä¿ç›¸ä¼¼åº¦åœ¨åˆç†ç¯„åœå…§
                similarity = max(0.0, min(1.0, similarity))
                search_results.append((metadata, similarity))

            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆé«˜åˆ°ä½ï¼‰
            search_results.sort(key=lambda x: x[1], reverse=True)

            return search_results
            
        except Exception as e:
            logger.error(f"æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

    def display_search_results(self, results: List[Tuple[Dict[str, Any], float]], query: str):
        """
        é¡¯ç¤ºæœå°‹çµæœ

        Args:
            results: æœå°‹çµæœ
            query: åŸå§‹æŸ¥è©¢
        """
        print(f"\nğŸ” æ­£åœ¨åŒ¹é…æœ€ç›¸è¿‘çš„å·¥ä½œæµç¨‹...")
        print(f"ğŸ“ æœå°‹é—œéµå­—ï¼š{query}")
        print("=" * 80)

        if not results:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„å·¥ä½œæµç¨‹")
            return

        for i, (metadata, similarity) in enumerate(results, 1):
            print(f"\n[{i}] ä¾†æºæª”æ¡ˆï¼š{metadata['filename']}")
            print(f"ğŸ§  ç›¸ä¼¼åº¦ï¼š{similarity:.4f}")
            print(f"ğŸ”§ å·¥ä½œæµç¨‹åç¨±ï¼š{metadata['name']}")
            print(f"ğŸ“Œ æ¨™ç±¤ï¼šworkflow")
            print(f"ğŸ“ è‡ªå®šæ¨™ç±¤ï¼š{metadata['name']}")
            print(f"ğŸ“– æè¿°ï¼š{metadata['description']}")
            print(f"\nğŸ§© Workflow çµæ§‹ï¼š")
            print()

            # é¡¯ç¤ºç¯€é»çµæ§‹
            node_structure_str = metadata.get('node_structure', '[]')
            try:
                # å˜—è©¦è§£æå­—ä¸²ç‚ºåˆ—è¡¨
                import ast
                if isinstance(node_structure_str, str):
                    node_structure = ast.literal_eval(node_structure_str)
                else:
                    node_structure = node_structure_str
            except:
                node_structure = []

            for j, node in enumerate(node_structure[:10]):  # æœ€å¤šé¡¯ç¤º10å€‹ç¯€é»
                if j == 0:
                    print(f"Triggerï¼š{node}")
                else:
                    print(f"{node}")

            if len(node_structure) > 10:
                print(f"... é‚„æœ‰ {len(node_structure) - 10} å€‹ç¯€é»")

            print()

    def generate_tutorial_markdown(self, metadata: Dict[str, Any], file_path: str) -> str:
        """
        ç”Ÿæˆæ•™å­¸èªªæ˜ Markdown æª”æ¡ˆ

        Args:
            metadata: å·¥ä½œæµç¨‹ metadata
            file_path: åŸå§‹ JSON æª”æ¡ˆè·¯å¾‘

        Returns:
            str: ç”Ÿæˆçš„ Markdown å…§å®¹
        """
        # è®€å–åŸå§‹ JSON æª”æ¡ˆä»¥ç²å–è©³ç´°è³‡è¨Š
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
        except Exception as e:
            logger.error(f"è®€å–æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
            workflow_data = {}

        # ç¯€é»è³‡æ–™ç›´æ¥åœ¨æ ¹å±¤ç´š
        nodes = workflow_data.get('nodes', [])

        # ç”Ÿæˆ Markdown å…§å®¹
        markdown_content = f"""# {metadata['name']} - å·¥ä½œæµç¨‹æ•™å­¸

## ğŸ“‹ åŸºæœ¬è³‡è¨Š

- **å·¥ä½œæµç¨‹åç¨±**: {metadata['name']}
- **å·¥ä½œæµç¨‹ ID**: {metadata.get('workflow_id', 'N/A')}
- **ç¯€é»æ•¸é‡**: {metadata.get('node_count', 0)}
- **ä¾†æºæª”æ¡ˆ**: {metadata['filename']}
- **ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“– å·¥ä½œæµç¨‹æè¿°

{metadata['description']}

"""

        # ç”Ÿæˆè©³ç´°è¨­å®šæ­¥é©Ÿ
        setup_steps = self.generate_setup_steps(nodes)

        # æ·»åŠ å»ºè­°ç”¨é€”
        markdown_content += f"""{setup_steps}

## ğŸ’¡ å»ºè­°ç”¨é€”èˆ‡å»¶ä¼¸æ‡‰ç”¨

### é©ç”¨å ´æ™¯
{self.generate_use_cases(nodes)}

### å»¶ä¼¸æ‡‰ç”¨å»ºè­°
{self.generate_extensions(nodes)}

## ğŸ”§ è¨­å®šæ³¨æ„äº‹é …

1. **èªè­‰è¨­å®š**: è«‹ç¢ºä¿æ‰€æœ‰éœ€è¦èªè­‰çš„ç¯€é»éƒ½å·²æ­£ç¢ºè¨­å®š API é‡‘é‘°æˆ–èªè­‰è³‡è¨Š
2. **è§¸ç™¼æ¢ä»¶**: æª¢æŸ¥è§¸ç™¼å™¨çš„è¨­å®šæ˜¯å¦ç¬¦åˆæ‚¨çš„éœ€æ±‚
3. **éŒ¯èª¤è™•ç†**: å»ºè­°ç‚ºé—œéµç¯€é»æ·»åŠ éŒ¯èª¤è™•ç†é‚è¼¯
4. **æ¸¬è©¦åŸ·è¡Œ**: åœ¨æ­£å¼ä½¿ç”¨å‰ï¼Œè«‹å…ˆé€²è¡Œæ¸¬è©¦åŸ·è¡Œ

## ğŸ“š ç›¸é—œè³‡æº

- [n8n å®˜æ–¹æ–‡æª”](https://docs.n8n.io/)
- [n8n ç¤¾ç¾¤ç¯„ä¾‹](https://n8n.io/workflows/)
- [ç¯€é»åƒè€ƒæ–‡æª”](https://docs.n8n.io/integrations/)

---
*æ­¤æ•™å­¸æª”æ¡ˆç”± n8n Workflow æ™ºèƒ½æœå°‹ç³»çµ±è‡ªå‹•ç”Ÿæˆ*
"""

        return markdown_content

    def sanitize_filename(self, filename: str) -> str:
        """
        æ¸…ç†æª”æ¡ˆåç¨±ï¼Œç§»é™¤ä¸åˆæ³•çš„å­—å…ƒ

        Args:
            filename: åŸå§‹æª”æ¡ˆåç¨±

        Returns:
            str: æ¸…ç†å¾Œçš„æª”æ¡ˆåç¨±
        """
        import re
        # ç§»é™¤æˆ–æ›¿æ›ä¸åˆæ³•çš„æª”æ¡ˆåç¨±å­—å…ƒ
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = re.sub(r'[ğŸ¤–ğŸš˜ğŸ“²ğŸ’¡ğŸ”§ğŸ“ŒğŸ“ğŸ“–ğŸ§©]', '', filename)  # ç§»é™¤ emoji
        filename = filename.strip()
        # é™åˆ¶é•·åº¦
        if len(filename) > 50:
            filename = filename[:50]
        return filename

    def get_node_description(self, node_type: str) -> str:
        """
        æ ¹æ“šç¯€é»é¡å‹å›å‚³åŠŸèƒ½æè¿°

        Args:
            node_type: ç¯€é»é¡å‹

        Returns:
            str: ç¯€é»åŠŸèƒ½æè¿°
        """
        descriptions = {
            'n8n-nodes-base.scheduleTrigger': 'å®šæ™‚è§¸ç™¼å™¨ï¼Œå¯è¨­å®šå®šæœŸåŸ·è¡Œçš„æ™‚é–“æ’ç¨‹',
            'n8n-nodes-base.httpRequest': 'HTTP è«‹æ±‚ç¯€é»ï¼Œç”¨æ–¼ç™¼é€ GET/POST ç­‰ HTTP è«‹æ±‚',
            'n8n-nodes-base.webhook': 'Webhook è§¸ç™¼å™¨ï¼Œæ¥æ”¶å¤–éƒ¨ HTTP è«‹æ±‚è§¸ç™¼å·¥ä½œæµç¨‹',
            'n8n-nodes-base.emailSend': 'éƒµä»¶ç™¼é€ç¯€é»ï¼Œç”¨æ–¼ç™¼é€é›»å­éƒµä»¶é€šçŸ¥',
            'n8n-nodes-base.discord': 'Discord æ•´åˆç¯€é»ï¼Œå¯ç™¼é€è¨Šæ¯åˆ° Discord é »é“',
            'n8n-nodes-base.slack': 'Slack æ•´åˆç¯€é»ï¼Œå¯ç™¼é€è¨Šæ¯åˆ° Slack é »é“',
            'n8n-nodes-base.telegram': 'Telegram Bot ç¯€é»ï¼Œå¯ç™¼é€è¨Šæ¯åˆ° Telegram',
            'n8n-nodes-base.code': 'ç¨‹å¼ç¢¼åŸ·è¡Œç¯€é»ï¼Œå¯åŸ·è¡Œ JavaScript ç¨‹å¼ç¢¼',
            'n8n-nodes-base.function': 'å‡½æ•¸ç¯€é»ï¼ŒåŸ·è¡Œè‡ªå®šç¾© JavaScript å‡½æ•¸',
            'n8n-nodes-base.if': 'æ¢ä»¶åˆ¤æ–·ç¯€é»ï¼Œæ ¹æ“šæ¢ä»¶æ±ºå®šåŸ·è¡Œè·¯å¾‘',
            'n8n-nodes-base.switch': 'åˆ†æ”¯ç¯€é»ï¼Œæ ¹æ“šä¸åŒæ¢ä»¶åŸ·è¡Œä¸åŒè·¯å¾‘',
            'n8n-nodes-base.merge': 'åˆä½µç¯€é»ï¼Œå°‡å¤šå€‹è³‡æ–™æµåˆä½µ',
            'n8n-nodes-base.set': 'è¨­å®šç¯€é»ï¼Œç”¨æ–¼è¨­å®šæˆ–ä¿®æ”¹è³‡æ–™',
            'n8n-nodes-base.wait': 'ç­‰å¾…ç¯€é»ï¼Œæš«åœå·¥ä½œæµç¨‹åŸ·è¡Œä¸€æ®µæ™‚é–“',
            'n8n-nodes-base.stickyNote': 'ä¾¿åˆ©è²¼ç¯€é»ï¼Œç”¨æ–¼æ·»åŠ è¨»è§£å’Œèªªæ˜',
            '@n8n/n8n-nodes-langchain.chainLlm': 'LangChain LLM ç¯€é»ï¼Œæ•´åˆå¤§å‹èªè¨€æ¨¡å‹',
        }

        return descriptions.get(node_type, 'è‡ªå®šç¾©ç¯€é»ï¼Œè«‹åƒè€ƒ n8n æ–‡æª”äº†è§£è©³ç´°åŠŸèƒ½')

    def generate_use_cases(self, nodes: List[Dict[str, Any]]) -> str:
        """
        æ ¹æ“šç¯€é»é¡å‹ç”Ÿæˆä½¿ç”¨å ´æ™¯å»ºè­°

        Args:
            nodes: ç¯€é»åˆ—è¡¨

        Returns:
            str: ä½¿ç”¨å ´æ™¯æè¿°
        """
        node_types = [node.get('type', '') for node in nodes]
        use_cases = []

        if any('schedule' in nt.lower() for nt in node_types):
            use_cases.append("- å®šæœŸè‡ªå‹•åŒ–ä»»å‹™ï¼ˆå¦‚æ¯æ—¥å ±å‘Šã€å®šæ™‚å‚™ä»½ï¼‰")

        if any('http' in nt.lower() for nt in node_types):
            use_cases.append("- API è³‡æ–™æ•´åˆèˆ‡åŒæ­¥")
            use_cases.append("- ç¬¬ä¸‰æ–¹æœå‹™æ•´åˆ")

        if any('email' in nt.lower() for nt in node_types):
            use_cases.append("- è‡ªå‹•åŒ–éƒµä»¶é€šçŸ¥ç³»çµ±")
            use_cases.append("- å ±å‘Šèˆ‡è­¦å‘Šç™¼é€")

        if any('discord' in nt.lower() or 'slack' in nt.lower() for nt in node_types):
            use_cases.append("- åœ˜éšŠå”ä½œé€šçŸ¥")
            use_cases.append("- ç³»çµ±ç›£æ§è­¦å‘Š")

        if any('webhook' in nt.lower() for nt in node_types):
            use_cases.append("- å³æ™‚äº‹ä»¶éŸ¿æ‡‰")
            use_cases.append("- å¤–éƒ¨ç³»çµ±æ•´åˆè§¸ç™¼")

        return "\n".join(use_cases) if use_cases else "- é€šç”¨è‡ªå‹•åŒ–å·¥ä½œæµç¨‹"

    def generate_extensions(self, nodes: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆå»¶ä¼¸æ‡‰ç”¨å»ºè­°

        Args:
            nodes: ç¯€é»åˆ—è¡¨

        Returns:
            str: å»¶ä¼¸æ‡‰ç”¨å»ºè­°
        """
        extensions = [
            "- æ·»åŠ éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶",
            "- æ•´åˆæ›´å¤šé€šçŸ¥ç®¡é“ï¼ˆå¦‚ LINEã€WeChatï¼‰",
            "- åŠ å…¥è³‡æ–™æŒä¹…åŒ–å­˜å„²",
            "- å¯¦ä½œæ¢ä»¶åˆ†æ”¯è™•ç†ä¸åŒæƒ…æ³",
            "- æ·»åŠ æ—¥èªŒè¨˜éŒ„å’Œç›£æ§åŠŸèƒ½"
        ]

        return "\n".join(extensions)

    def generate_setup_steps(self, nodes: List[Dict[str, Any]]) -> str:
        """
        æ ¹æ“šå¯¦éš›ç¯€é»ç”Ÿæˆè©³ç´°çš„è¨­å®šæ­¥é©Ÿ

        Args:
            nodes: ç¯€é»åˆ—è¡¨

        Returns:
            str: è¨­å®šæ­¥é©Ÿèªªæ˜
        """
        steps = []
        step_count = 1

        # åˆ†æå·¥ä½œæµç¨‹çš„å¯¦éš›çµæ§‹ï¼Œç”Ÿæˆå°æ‡‰çš„æ“ä½œæ­¥é©Ÿ
        for i, node in enumerate(nodes, 1):
            node_name = node.get('name', f'ç¯€é» {i}')
            node_type = node.get('type', 'unknown')
            parameters = node.get('parameters', {})

            steps.append(f"### æ­¥é©Ÿ {step_count}ï¼šè¨­å®š {node_name}")
            step_count += 1

            # æ ¹æ“šç¯€é»é¡å‹ç”Ÿæˆå…·é«”çš„è¨­å®šæ­¥é©Ÿ
            if 'trigger' in node_type.lower():
                if 'schedule' in node_type.lower():
                    steps.append("1. åœ¨å·¥ä½œæµç¨‹çš„ç•«å¸ƒä¸Šï¼Œæ‹–æ›³ã€ŒSchedule Triggerã€ç¯€é»")
                    steps.append("2. é»æ“Šç¯€é»ï¼Œä¸¦è¨­å®šè§¸ç™¼é »ç‡ï¼š")
                    steps.append("   - **Mode**: é¸æ“‡ã€ŒEvery Dayã€")
                    steps.append("   - **Time**: è¨­å®šæ‚¨å¸Œæœ›è§¸ç™¼çš„æ™‚é–“ï¼Œä¾‹å¦‚ã€Œ08:00ã€")
                    steps.append("3. é»æ“Šã€ŒExecute Nodeã€ä¾†æ¸¬è©¦é€™å€‹ç¯€é»ï¼Œç¢ºä¿å®ƒèƒ½æ­£å¸¸é‹ä½œ")
                elif 'telegram' in node_type.lower():
                    steps.append("1. æ‹–æ›³ã€ŒTelegram Triggerã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                    steps.append("2. è¨­å®š Telegram Bot è§¸ç™¼å™¨ï¼š")
                    if 'updates' in parameters:
                        updates = parameters.get('updates', [])
                        if 'message' in updates:
                            steps.append("   - **Updates**: é¸æ“‡ã€Œmessageã€ä¾†æ¥æ”¶è¨Šæ¯")
                        if 'callback_query' in updates:
                            steps.append("   - **Updates**: é¸æ“‡ã€Œcallback_queryã€ä¾†æ¥æ”¶æŒ‰éˆ•å›èª¿")
                    steps.append("3. åœ¨æ†‘è­‰ç®¡ç†ä¸­è¨­å®š Telegram Bot Token")
                elif 'webhook' in node_type.lower():
                    steps.append("1. æ‹–æ›³ã€ŒWebhookã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                    steps.append("2. è¨­å®š Webhook åƒæ•¸ï¼š")
                    steps.append("   - **HTTP Method**: é¸æ“‡é©ç•¶çš„æ–¹æ³•ï¼ˆGET/POSTï¼‰")
                    steps.append("   - **Path**: è¨­å®š webhook è·¯å¾‘")
                    steps.append("3. å„²å­˜ä¸¦å•Ÿç”¨å·¥ä½œæµç¨‹ä»¥ç²å¾— webhook URL")
                elif 'gmail' in node_type.lower():
                    steps.append("1. æ‹–æ›³ã€ŒGmail Triggerã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                    steps.append("2. è¨­å®š Gmail è§¸ç™¼å™¨ï¼š")
                    steps.append("   - **Event**: é¸æ“‡è§¸ç™¼äº‹ä»¶ï¼ˆå¦‚æ–°éƒµä»¶ï¼‰")
                    steps.append("3. åœ¨æ†‘è­‰ç®¡ç†ä¸­è¨­å®š Gmail OAuth2 èªè­‰")

            elif 'httpRequest' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒHTTP Requestã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. é»æ“Šç¯€é»ï¼Œç„¶å¾Œè¨­å®šï¼š")
                if 'method' in parameters:
                    method = parameters.get('method', 'GET')
                    steps.append(f"   - **Method**: é¸æ“‡ã€Œ{method}ã€")
                if 'url' in parameters:
                    steps.append("   - **URL**: è¼¸å…¥ API ç«¯é» URL")
                if 'sendHeaders' in parameters or 'headerParameters' in parameters:
                    steps.append("   - **Headers**: è¨­å®šå¿…è¦çš„è«‹æ±‚æ¨™é ­")
                if 'sendBody' in parameters:
                    steps.append("   - **Body**: è¨­å®šè«‹æ±‚ä¸»é«”å…§å®¹")
                steps.append("3. æ¸¬è©¦ç¯€é»ç¢ºä¿ API è«‹æ±‚æ­£å¸¸é‹ä½œ")

            elif 'telegram' in node_type.lower() and 'trigger' not in node_type.lower():
                steps.append("1. æ‹–æ›³ã€ŒTelegramã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®š Telegram è¨Šæ¯ç™¼é€ï¼š")
                if 'text' in parameters:
                    steps.append("   - **Text**: è¨­å®šè¦ç™¼é€çš„è¨Šæ¯å…§å®¹")
                if 'chatId' in parameters:
                    steps.append("   - **Chat ID**: è¨­å®šç›®æ¨™èŠå¤©å®¤ ID")
                if 'resource' in parameters and parameters.get('resource') == 'file':
                    steps.append("   - **Resource**: é¸æ“‡ã€Œfileã€ä¾†ä¸‹è¼‰æª”æ¡ˆ")
                    if 'fileId' in parameters:
                        steps.append("   - **File ID**: è¨­å®šè¦ä¸‹è¼‰çš„æª”æ¡ˆ ID")
                steps.append("3. åœ¨æ†‘è­‰ç®¡ç†ä¸­è¨­å®š Telegram Bot Token")

            elif 'googleSheets' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒGoogle Sheetsã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®š Google Sheets æ“ä½œï¼š")
                if 'operation' in parameters:
                    operation = parameters.get('operation', 'read')
                    steps.append(f"   - **Operation**: é¸æ“‡ã€Œ{operation}ã€")
                if 'documentId' in parameters:
                    steps.append("   - **Document ID**: è¼¸å…¥ Google Sheets æ–‡ä»¶ ID")
                if 'sheetName' in parameters:
                    steps.append("   - **Sheet Name**: é¸æ“‡è¦æ“ä½œçš„å·¥ä½œè¡¨")
                steps.append("3. åœ¨æ†‘è­‰ç®¡ç†ä¸­è¨­å®š Google Sheets OAuth2 èªè­‰")

            elif 'code' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒCodeã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®š JavaScript ç¨‹å¼ç¢¼ï¼š")
                steps.append("   - **JavaScript Code**: è¼¸å…¥è‡ªå®šç¾©çš„è™•ç†é‚è¼¯")
                steps.append("3. æ¸¬è©¦ç¨‹å¼ç¢¼ç¢ºä¿é‚è¼¯æ­£ç¢º")

            elif 'set' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒSetã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®šè³‡æ–™è™•ç†ï¼š")
                if 'assignments' in parameters:
                    steps.append("   - **Assignments**: è¨­å®šè¦ä¿®æ”¹æˆ–æ–°å¢çš„è³‡æ–™æ¬„ä½")
                steps.append("3. æ¸¬è©¦ç¯€é»ç¢ºä¿è³‡æ–™è™•ç†æ­£ç¢º")

            elif 'switch' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒSwitchã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®šæ¢ä»¶åˆ†æ”¯ï¼š")
                if 'rules' in parameters:
                    steps.append("   - **Rules**: è¨­å®šåˆ¤æ–·æ¢ä»¶å’Œå°æ‡‰çš„è¼¸å‡ºè·¯å¾‘")
                steps.append("3. æ¸¬è©¦å„å€‹åˆ†æ”¯è·¯å¾‘")

            elif 'wait' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒWaitã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®šç­‰å¾…æ™‚é–“ï¼š")
                if 'amount' in parameters:
                    amount = parameters.get('amount', 1)
                    steps.append(f"   - **Amount**: è¨­å®šç­‰å¾…æ™‚é–“ç‚º {amount} ç§’")
                steps.append("3. æ ¹æ“šéœ€è¦èª¿æ•´ç­‰å¾…æ™‚é–“")

            elif 'merge' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒMergeã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®šè³‡æ–™åˆä½µæ–¹å¼ï¼š")
                if 'mode' in parameters:
                    mode = parameters.get('mode', 'append')
                    steps.append(f"   - **Mode**: é¸æ“‡ã€Œ{mode}ã€åˆä½µæ¨¡å¼")
                steps.append("3. é€£æ¥å¤šå€‹è¼¸å…¥ç¯€é»åˆ°æ­¤åˆä½µç¯€é»")

            elif 'splitOut' in node_type:
                steps.append("1. æ‹–æ›³ã€ŒSplit Outã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®šè³‡æ–™åˆ†å‰²ï¼š")
                if 'fieldToSplitOut' in parameters:
                    field = parameters.get('fieldToSplitOut', '')
                    steps.append(f"   - **Field to Split Out**: è¨­å®šè¦åˆ†å‰²çš„æ¬„ä½ã€Œ{field}ã€")
                steps.append("3. æ¸¬è©¦è³‡æ–™åˆ†å‰²çµæœ")

            elif 'langchain' in node_type.lower():
                steps.append("1. æ‹–æ›³ LangChain ç›¸é—œç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. è¨­å®š AI æ¨¡å‹åƒæ•¸ï¼š")
                if 'model' in parameters:
                    model = parameters.get('model', {})
                    if isinstance(model, dict) and 'value' in model:
                        steps.append(f"   - **Model**: é¸æ“‡ã€Œ{model['value']}ã€")
                if 'text' in parameters:
                    steps.append("   - **Text**: è¨­å®šè¼¸å…¥æ–‡æœ¬")
                if 'messages' in parameters:
                    steps.append("   - **Messages**: è¨­å®šå°è©±è¨Šæ¯")
                steps.append("3. åœ¨æ†‘è­‰ç®¡ç†ä¸­è¨­å®šç›¸æ‡‰çš„ API é‡‘é‘°")

            else:
                # é€šç”¨è¨­å®šæ­¥é©Ÿ
                steps.append(f"1. æ‹–æ›³ã€Œ{node_name}ã€ç¯€é»åˆ°å·¥ä½œæµç¨‹ç•«å¸ƒä¸Š")
                steps.append("2. é»æ“Šç¯€é»é€²è¡Œè¨­å®šï¼š")
                if parameters:
                    main_params = list(parameters.keys())[:3]  # é¡¯ç¤ºå‰3å€‹ä¸»è¦åƒæ•¸
                    for param in main_params:
                        steps.append(f"   - **{param}**: æ ¹æ“šéœ€æ±‚è¨­å®šæ­¤åƒæ•¸")
                steps.append("3. æ¸¬è©¦ç¯€é»ç¢ºä¿è¨­å®šæ­£ç¢º")

            steps.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”

        if not steps:
            return ""

        return f"""## ğŸ› ï¸ è©³ç´°è¨­å®šæ­¥é©Ÿ

{chr(10).join(steps)}"""

    def save_selected_workflow(self, metadata: Dict[str, Any], index: int = 0) -> bool:
        """
        å„²å­˜é¸ä¸­çš„å·¥ä½œæµç¨‹

        Args:
            metadata: å·¥ä½œæµç¨‹ metadata
            index: é¸æ“‡çš„ç´¢å¼•

        Returns:
            bool: æ˜¯å¦å„²å­˜æˆåŠŸ
        """
        try:
            source_file = metadata['file_path']
            original_filename = metadata['filename']
            workflow_name = metadata['name']

            # æ¸…ç†æª”æ¡ˆåç¨±ä¸­çš„ç‰¹æ®Šå­—å…ƒ
            safe_workflow_name = self.sanitize_filename(workflow_name)
            safe_original_name = os.path.splitext(original_filename)[0]  # ç§»é™¤ .json å‰¯æª”å

            # è¤‡è£½ JSON æª”æ¡ˆåˆ° selected_workflows ç›®éŒ„ï¼Œä½¿ç”¨æ–°çš„å‘½åæ ¼å¼
            new_json_filename = f"workflow_{safe_original_name}.json"
            dest_json = os.path.join(self.selected_dir, new_json_filename)
            shutil.copy2(source_file, dest_json)

            # ç”Ÿæˆæ•™å­¸ Markdown æª”æ¡ˆï¼Œä½¿ç”¨å·¥ä½œæµç¨‹åç¨±
            tutorial_filename = f"workflow_{safe_workflow_name}.md"
            tutorial_path = os.path.join(self.selected_dir, tutorial_filename)

            tutorial_content = self.generate_tutorial_markdown(metadata, source_file)

            with open(tutorial_path, 'w', encoding='utf-8') as f:
                f.write(tutorial_content)

            print(f"\nâœ… æˆåŠŸå„²å­˜å·¥ä½œæµç¨‹ï¼")
            print(f"ğŸ“ JSON æª”æ¡ˆï¼š{dest_json}")
            print(f"ğŸ“š æ•™å­¸æª”æ¡ˆï¼š{tutorial_path}")

            return True

        except Exception as e:
            logger.error(f"å„²å­˜å·¥ä½œæµç¨‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"âŒ å„²å­˜å¤±æ•—ï¼š{e}")
            return False

    def interactive_search(self):
        """
        äº’å‹•å¼æœå°‹ä»‹é¢
        """
        print("=" * 80)
        print("ğŸ¤– n8n Workflow æ™ºèƒ½æœå°‹èˆ‡æ•™å­¸ç”Ÿæˆç³»çµ±")
        print("=" * 80)
        print("ğŸ’¡ è«‹æè¿°æ‚¨æƒ³è¦å¯¦ä½œçš„å·¥ä½œæµç¨‹ï¼Œä¾‹å¦‚ï¼š")
        print("   â€¢ æ¯å¤©æ—©ä¸Šå¯„å‡ºæ°£è±¡é€šçŸ¥")
        print("   â€¢ ç›£æ§ä¼ºæœå™¨ç•°å¸¸ç™¼ Discord è­¦å‘Š")
        print("   â€¢ å®šæ™‚å‚™ä»½è³‡æ–™åº«ä¸¦ç™¼é€å ±å‘Š")
        print("=" * 80)

        while True:
            try:
                # å–å¾—ä½¿ç”¨è€…è¼¸å…¥
                query = input("\nğŸ” è«‹è¼¸å…¥æ‚¨çš„éœ€æ±‚æè¿°ï¼ˆè¼¸å…¥ 'quit' çµæŸï¼‰: ").strip()

                if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼")
                    break

                if not query:
                    print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æè¿°")
                    continue

                # åŸ·è¡Œæœå°‹
                print(f"\nğŸ” æ­£åœ¨æœå°‹ç›¸é—œå·¥ä½œæµç¨‹...")
                results = self.search_workflows(query, top_k=5)

                if not results:
                    print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„å·¥ä½œæµç¨‹ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—")
                    continue

                # é¡¯ç¤ºæœå°‹çµæœ
                self.display_search_results(results, query)

                # è®“ä½¿ç”¨è€…é¸æ“‡
                while True:
                    try:
                        choice = input(f"\nğŸ“¥ è«‹é¸æ“‡è¦å„²å­˜çš„å·¥ä½œæµç¨‹ç·¨è™Ÿ (1-{len(results)})ï¼Œæˆ–è¼¸å…¥ 'back' é‡æ–°æœå°‹: ").strip()

                        if choice.lower() in ['back', 'b', 'è¿”å›']:
                            break

                        choice_idx = int(choice) - 1
                        if 0 <= choice_idx < len(results):
                            metadata, _ = results[choice_idx]  # similarity ä¸éœ€è¦ä½¿ç”¨

                            # ç¢ºèªå„²å­˜
                            confirm = input(f"ç¢ºå®šè¦å„²å­˜ã€Œ{metadata['name']}ã€å—ï¼Ÿ(y/n): ").strip().lower()
                            if confirm in ['y', 'yes', 'æ˜¯', 'ok']:
                                if self.save_selected_workflow(metadata, choice_idx + 1):
                                    print("\nğŸ‰ å·¥ä½œæµç¨‹å·²æˆåŠŸå„²å­˜åˆ° selected_workflows/ ç›®éŒ„")
                                break
                            else:
                                print("âŒ å·²å–æ¶ˆå„²å­˜")
                        else:
                            print(f"âŒ è«‹è¼¸å…¥ 1-{len(results)} ä¹‹é–“çš„æ•¸å­—")

                    except ValueError:
                        print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
                    except KeyboardInterrupt:
                        print("\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·")
                        return

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·")
                break
            except Exception as e:
                logger.error(f"åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰ workflow æª”æ¡ˆ
        current_dir = "."
        json_files = []
        for _, _, files in os.walk(current_dir):
            for file in files:
                if file.endswith('.json') and not file.startswith('.'):
                    json_files.append(file)

        if not json_files:
            print("âŒ åœ¨ç•¶å‰ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆ")
            print("ğŸ’¡ è«‹ç¢ºä¿æ‚¨çš„ workflow JSON æª”æ¡ˆåœ¨ç•¶å‰ç›®éŒ„æˆ–å­ç›®éŒ„ä¸­")
            return

        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")

        # åˆå§‹åŒ–æœå°‹ç³»çµ±
        searcher = N8nWorkflowSearcher()

        # é–‹å§‹äº’å‹•å¼æœå°‹
        searcher.interactive_search()

    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        print(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()
